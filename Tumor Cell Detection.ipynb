{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Cell Detection with Tensorflow \n",
    "\n",
    "This is a study for identification of different cell types in biological tissue images. There are many types of cells present in a tissue. Some can be cancerous cells, immune cells while many others which look like a cell but actually not cell at all. This excercise uses Tensorflow to classify a valid cell from a non cell object from a given tissue image. \n",
    "\n",
    "I have collected some Immuno Histological Chemistry (IHC) stained images of brain tumor cells. I wrote an pre-processing algorithm to dentify the cell regions for a given stained image. In this project, I have developed an automatic cell detection framework using Tensorflow Object Detection API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "The initial dataset consist of 110 svs wholeslide images of brain tumor cell. The cells, which are dark circular regions comes in variety of shapes and sizes and are stained differently. To bring all images in same color, I have normalized the color coordinates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table> \n",
    "<tr>\n",
    "<td colspan=\"3\"> <img src=\"Images_Jupyter/whole_slide_image.png\" alt=\"Drawing\" style=\"width: 600px; height:300px\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"3\"> <div align=\"center\"> IHC stained whole slide image sample </div> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <img src=\"Images_Jupyter/image_patch1.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"Images_Jupyter/image_patch2.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"Images_Jupyter/image_patch3.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"3\"> <div align=\"center\"> Extracted Image Patches </div> </td>  \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "First, small patches are extracted from raw images in MATLAB. The corresponding code is attached here:\n",
    "\n",
    "<img src=\"Images_Jupyter/patch_generate.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have generated semi-hand annotations for the cell regions in each patch. Implemented a crude algorithm for cell detection and then further refined by hand(by removing the false positive). The accuracy of this annotation is only ($~85\\%$).\n",
    "\n",
    "<img src=\"Images_Jupyter/matlab_BB_anno.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rsulting patches looks like: \n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"Images_Jupyter/train_image1_BB_anno.png\" style=\"width: 200px;\"> </td>\n",
    "        <td> <img src=\"Images_Jupyter/train_image2_BB_anno.png\" style=\"width: 200px;\"> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\"> <div align=\"center\"> Generated cell patches </div> </td>\n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "import json\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment, tostring\n",
    "from ElementTree_pretty import prettify, xml_write\n",
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Inputs: Dataset and annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used [bmidb0 cluster](http://bmidb.cs.stonybrook.edu/) GPU for current study and cloned the models folder into the repository root directory. I have installed the TensorFlow API by running the following commands from root directory. My working folder is ../tensorflow/models/research"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## reference: https://github.com/wagonhelm/TF_ObjectDetection_API/blob/master/ChessObjectDetection.ipynb\n",
    "\n",
    "git clone https://github.com/tensorflow/models.git\n",
    "cd models/research/\n",
    "mkdir protoc_3.3\n",
    "cd protoc_3.3\n",
    "wget wget https://github.com/google/protobuf/releases/download/v3.3.0/protoc-3.3.0-linux-x86_64.zip\n",
    "chmod 775 protoc-3.3.0-linux-x86_64.zip\n",
    "unzip protoc-3.3.0-linux-x86_64.zip\n",
    "cd ../models/research/\n",
    "/data/scratch/mnroy/DeepLearning/Project1/tensorflow/protoc_3.3/bin/protoc object_detection/protos/*.proto --python_out=.\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraint, I could have generated the ground truth annotations of 50 images, used for the  training/validation. The corresponding images are saved in ../research/images/train folder. Since, the bounding box annotations were saved in json file in MATLAB, I have written scripts to convert them into standard PASCAL VOC xml format from json. The xml annotation files are saved in ../research/annotations folder. After that, the xml files are converted to csv by runnning the script xml_to_csv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert json to XML annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference site to write xml file from json data: https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
    "\n",
    "\n",
    "'''ElementTree_pretty.py '''\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "\n",
    "def xml_write(elem, filename):\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    with open(filename,'w') as write_file:\n",
    "        write_file.write(reparsed.toprettyxml(indent=\"  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''createXmlFromJson.py''' \n",
    "'''This function will write the bounding box annotations along with other informations in xml \n",
    "files similar of PASCAL VOC format'''\n",
    "\n",
    "Path = \"/data/scratch/mnroy/DeepLearning/Project1/tensorflow/models/research/anno_json/\"\n",
    "filelist = os.listdir(Path)\n",
    "for i in filelist:\n",
    "    if i.endswith(\".json\"):  # You could also add \"and i.startswith('f')\n",
    "        with open(Path + i, 'r') as data_file:\n",
    "            data = json.load(data_file)\n",
    "            \n",
    "    filename = \"/data/scratch/mnroy/DeepLearning/Project1/tensorflow/models/research/annotations/\" + \n",
    "    data['filename'].split('.')[0] + '.xml'\n",
    "\n",
    "    xml = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "    '''\n",
    "    top = Element('annotation')\n",
    "    child1 = SubElement(top, 'folder')\n",
    "    child1.text = data['folder']\n",
    "    child2 = SubElement(top, 'filename')\n",
    "    child2.text = data['filename']\n",
    "\n",
    "    child3 = SubElement(top, 'path')\n",
    "    child3.text = \"/data/scratch/mnroy/DeepLearning/Project1/tensorflow/models/research/images/train/\" + \n",
    "    data['filename']\n",
    "\n",
    "    source = SubElement(top, 'source')\n",
    "    database = SubElement(source, 'database')\n",
    "    database.text = \"Unknown\"\n",
    "    size = SubElement(top, 'size')\n",
    "\n",
    "    width = SubElement(size, 'width')\n",
    "    width.text = str(data['size'][0])\n",
    "\n",
    "    height = SubElement(size, 'height')\n",
    "    height.text = str(data['size'][1])\n",
    "\n",
    "    depth = SubElement(size, 'depth')\n",
    "    depth.text = str(data['size'][2]) \n",
    "\n",
    "    child4 = SubElement(top, 'segmented')\n",
    "    child4.text = str(0)\n",
    "\n",
    "    #for k in range(len(data['bndbox'])):\n",
    "    for bndbox,img_cls in zip(data['bndbox'],data['image_class']):\n",
    "        obj = SubElement(top, 'object')\n",
    "\n",
    "        name = SubElement(obj, 'name')\n",
    "        if img_cls:\n",
    "                    name.text = 'Cell'\n",
    "        else:\n",
    "                    name.text='Non-Cell'\n",
    " \n",
    "        pose = SubElement(obj, 'pose')\n",
    "        pose.text = 'Unspecified'\n",
    "        truncated = SubElement(obj, 'truncated')\n",
    "        truncated.text = str(0)\n",
    "        difficult = SubElement(obj, 'difficult')\n",
    "        difficult.text = str(0)\n",
    "        bndbox = SubElement(obj, 'bndbox')\n",
    "        xmin = SubElement(bndbox, 'xmin')\n",
    "        xmin.text = str(bndbox[0])\n",
    "        ymin = SubElement(bndbox, 'ymin')\n",
    "        ymin.text = str(bndbox[1])\n",
    "        xmax = SubElement(bndbox, 'xmax')\n",
    "        xmax.text = str(bndbox[2])\n",
    "        ymax = SubElement(bndbox, 'ymax')\n",
    "        ymax.text = str(bndbox[3])\n",
    "\n",
    "    xml_write(top, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert XML Labels to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from source: https://github.comr/datitran/raccoon_dataset/blob/master/xml_to_csv.py\n",
    "\n",
    "def xml_to_csv(path):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(path + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('filename').text,\n",
    "                     int(root.find('size')[0].text),\n",
    "                     int(root.find('size')[1].text),\n",
    "                     member[0].text,\n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "                     int(member[4][3].text)\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    return xml_df\n",
    "\n",
    "def main():\n",
    "    image_path = os.path.join(os.getcwd(), 'annotations')\n",
    "    xml_df = xml_to_csv(image_path)\n",
    "    xml_df.to_csv('data/cell_labels.csv', index=None)\n",
    "    print('Successfully converted xml to csv.')\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Maps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label map associated with the dataset defining a mapping from string class names to integer class Ids, are stored in ../research/data/cell_label_map.pbtxt. The label map of my dataset corersponds to two classes, Cell and Non-Cell. It is very important to start the label map from id 1, since the index 0 is a placeholder index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item {\n",
    "  id: 1\n",
    "  name: 'Cell'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'Non-Cell'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split labels.ipynb : used to split the full labels into train and test labels. \n",
    "## Reference: https://github.com/datitran/raccoon_dataset/blob/master/split%20labels.ipynb\n",
    "\n",
    "np.random.seed(1)\n",
    "full_labels = pd.read_csv('data/cell_labels.csv')\n",
    "\n",
    "full_labels.head()\n",
    "grouped = full_labels.groupby('filename')\n",
    "grouped.apply(lambda x: len(x)).value_counts()\n",
    "full_labels.count()\n",
    "total_data = full_labels['filename']\n",
    "\n",
    "gb = full_labels.groupby('filename')\n",
    "grouped_list = [gb.get_group(x) for x in gb.groups]\n",
    "len(grouped_list)\n",
    "\n",
    "train_index = np.random.choice(len(grouped_list), size=40, replace=False)\n",
    "test_index = np.setdiff1d(list(range(50)), train_index)\n",
    "len(train_index), len(test_index)\n",
    "\n",
    "# take first 10 files\n",
    "train = pd.concat([grouped_list[i] for i in train_index])\n",
    "test = pd.concat([grouped_list[i] for i in test_index])\n",
    "len(train), len(test)\n",
    "train.to_csv('data/train_labels.csv', index=None)\n",
    "test.to_csv('data/test_labels.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Record \n",
    "\n",
    "The Tensorflow Object Detection API expects data to be in the TFRecord format, so we'll now run the create_cell_tf_record script to convert from the raw  dataset into TFRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md\n",
    "\n",
    "\"\"\"\n",
    "Usage:\n",
    "  # From tensorflow/models/research\n",
    "  # Create train data:\n",
    "  python create_Cell_tf_record.py --csv_input=data/train_labels.csv  --output_path=data/train.record\n",
    "  # Create test data:\n",
    "  python create_Cell_tf_record.py --csv_input=data/test_labels.csv  --output_path=data/test.record\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
    "flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# TO-DO replace this with label map\n",
    " def class_text_to_int(row_label):\n",
    "    if row_label == 'Cell':\n",
    "        return 1\n",
    "\n",
    "    if row_label == 'Non-Cell':\n",
    "        return 2 \n",
    "    else:\n",
    "        None\n",
    "   \n",
    " def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    " def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_png = fid.read()\n",
    "    encoded_png_io = io.BytesIO(encoded_png)\n",
    "    image = Image.open(encoded_png_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'png'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        #print(class_text_to_int(row['class']))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_png),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    " def main(_):\n",
    "    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
    "    path = os.path.join(os.getcwd(), 'images/train')\n",
    "    examples = pd.read_csv(FLAGS.csv_input)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model for Transfer Learning\n",
    "\n",
    "Training an object detector from scratch can take days, even when using multiple GPUs!. For my work, accuracy of Cell locations is more important than speed of training. Therefore, I have used Faster-RCNN-ResNet object detection model trained on the [COCO dataset](http://cocodataset.org/#home), and reused some of it's parameters to initialize my new model. I have downloaded COCO-pretrained Faster R-CNN with Resnet-101 model and unzipped the contents of the folder and copied the model.ckpt* files into data folder. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget http://storage.googleapis.com/download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_11_06_2017.tar.gz\n",
    "tar -xvf faster_rcnn_resnet101_coco_11_06_2017.tar.gz\n",
    "cp faster_rcnn_resnet101_coco_11_06_2017/model.ckpt.* data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Object Detection Training Pipeline \n",
    "\n",
    "I have configured the [config](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs) file for resnet101 and saved as ../resaerch/data/faster_rcnn_resnet101_Cell.config.\n",
    "I have changed the num_classes to two and also set the path (PATH_TO_BE_CONFIGURED) for the model checkpoint, the train and test data files as well as the label map. The other configurations like the learning rate, batch size etc., I kept their default settings. I have used num_steps as 100000. I have used data_augmentation_options (i) random_vertical_flip{} and (ii) random_rotation90{} apart from their default one, random_horizontal_flip{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I do have the training/validation datasets (including label map), COCO trained FasterRCNN finetune checkpoint and configuration file in the ../research/data folder, which should look like the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+ $../research/\n",
    "  + data/\n",
    "    - faster_rcnn_resnet101_Cell.config\n",
    "    - model.ckpt.index\n",
    "    - model.ckpt.meta\n",
    "    - model.ckpt.data-00000-of-00001\n",
    "    - cell_label_map.pbtxt \n",
    "    - train.record\n",
    "    - test.record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Since, I have used Transfer learning from the pre trained model, the training time is much less. I have used cluster GPU(node0) for training and CPU for evaluation at the same time. I have run the follwoing code for traning and evaluation in two different screens in the repository root directory. It is much easier to monitor the process of the training and evaluation jobs by running Tensorboard on the cluster. Once the loss drops to a consistant level for a while, TensorFlow training can be stopped by pressing ctrl+c.\n",
    "To start training and evaluation, the following commands are executed from the tensorflow/models/research/  directory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd ../models/research/\n",
    "/data/scratch/mnroy/DeepLearning/Project1/tensorflow/protoc_3.3/bin/protoc object_detection/protos/*.proto --python_out=.\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "\n",
    "screen1\n",
    "python object_detection/train.py \\\n",
    "    --logtostderr \\\n",
    "    --pipeline_config_path=data/faster_rcnn_resnet101_Cell.config \\\n",
    "    --train_dir=train\n",
    "\n",
    "\n",
    "screen2 \n",
    "python object_detection/eval.py \\\n",
    "    --logtostderr \\\n",
    "    --pipeline_config_path=data/faster_rcnn_resnet101_Cell.config \\\n",
    "    --checkpoint_dir=train \\\n",
    "    --eval_dir=eval    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Progress with Tensorboard \n",
    "\n",
    "I have started the TensorBoard in bmidb0 cluster to monitor the total loss and other evaluation parameters, by running the below command from ../tensflow/models directory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensorboard --logdir='research' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"Images_Jupyter/tensor_board.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of result \n",
    "\n",
    "Here are the results from my training and evaluation jobs. In total, I ran it over few hours/60k steps with a batch size of 1 (due to GPU memory limitations), but I already achieved reasonable results after about 30k steps.\n",
    "This is how the total loss evolved. Total loss decreased quite fast due to the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"Images_Jupyter/Total_Loss.png\" width=\"800\"> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <div align=\"center\"> Total Loss saturating at ~30k steps </div> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mAP (mean average precision) at 0.5IoU for Cell, Non-Cell and total are shown below. Since the Cell regions are main focus of this study, the mAP value for Cell 0.7590 at 38k step is quite good to achieve. The Non-Cell regions are quite a bit unclear from the images, therefore the mAP estimate is quite low resulting in lower value for total mAP estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"Images_Jupyter/tensorboard_mAP.png\" width=\"800\"> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> <div align=\"center\"> mAP estimates </div> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for the evaluation of few images while training the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td> <img src=\"Images_Jupyter/Initial_eval_step0.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"Images_Jupyter/Images_evalEnd.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td> <div align=\"center\"> Initial evaluation at step 0 of the training model. </div> </td>\n",
    "      <td> <div align=\"center\"> Evaluation at end of training. </div> </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection Utilities copied to Root Directory \n",
    "\n",
    "I have copied some of the utilities from the Object Detection folder into the root directory."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cp -R object_detection/utils/. utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the trained model for inference \n",
    "\n",
    "After the model has been trained, it is required to export it to a Tensorflow graph proto. First I have created a directory exported_graphs in ../research folder where output_inference_graph.pb will be saved. Then, I have identified a candidate checkpoint from the file stored at ../research/train folder followed by executing the below command from tensorflow/models/research/:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# From tensorflow/models/research/\n",
    "python object_detection/export_inference_graph.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path data/faster_rcnn_resnet101_Cell.config \\\n",
    "    --trained_checkpoint_prefix train/model.ckpt-19267 \\\n",
    "    --output_directory exported_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified From API\n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# This is needed to display the images.\n",
    "#%matplotlib inline\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = 'exported_graphs/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'data/cell_label_map.pbtxt'\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "PATH_TO_TEST_IMAGES_DIR = 'images/test'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.png'.format(i)) for i in range(1, 10) ]\n",
    "IMAGE_SIZE = (8, 8)\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "with detection_graph.as_default():\n",
    "      with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        for image_path in TEST_IMAGE_PATHS:\n",
    "            image = Image.open(image_path)\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            # Actual detection.\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "              image_np,\n",
    "              np.squeeze(boxes),\n",
    "              np.squeeze(classes).astype(np.int32),\n",
    "              np.squeeze(scores),\n",
    "              category_index,\n",
    "              use_normalized_coordinates=True,\n",
    "              line_thickness=1)\n",
    "            plt.figure(figsize=IMAGE_SIZE)\n",
    "            fig = plt.imshow(image_np)\n",
    "            fig.set_cmap('hot')\n",
    "            plt.axis('off')\n",
    "            fig.axes.get_xaxis().set_visible(False)\n",
    "            fig.axes.get_yaxis().set_visible(False)\n",
    "            #plt.show()\n",
    "\n",
    "            filename = 'test_result/' + image_path.split('/')[-1]\n",
    "            #filename =  image_path.split('/')[-1]\n",
    "            plt.savefig(filename, bbox_inches='tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Result Images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"Images_Jupyter/image8.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td> <img src=\"Images_Jupyter/image7.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td> <img src=\"Images_Jupyter/image6.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "    <td> <img src=\"Images_Jupyter/image3.png\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td colspan=\"4\"> <div align=\"center\"> Test Images </div> </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From the above study, we can infer that Faster rcnn resnet model is performing quite well on unseen images and able to detect most of the Cell locations. Although, its performance degrades when numerous number of Cells are present in the image patch. This is due to very limited number of training data and the ground truth accuracy of bounding box annotations is also quite less. This is a research project, I will be pursuing further on this topic and prepare it for publication in near future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
